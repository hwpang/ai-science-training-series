# HW7 hwpang

## Cerebas

The output content can be found in `Cerebas.out`. I used a batch size of 2048. With the larger batch size, the training time becomes shorter compared to the batch size of 1024 during the class. This is because now we need less step to loop through the dataset.

## Graphcore

The output content can be found in `Graphcore.out`. I changed the epochs from 10 to 20 on the mnist example. Compared to the sample output, the test set accuracy increases from 96.85% to 98.16%. This suggests that the model is actually underfitting the dataset, and learning more epochs can help improve the accuracy.

## Groq

The output content can be found in `Groq.out`. I changed the max sequence length from 128 to 256. I expected using a larger max sequence should decrease the wall time as it can iterate through the data earlier, but surprisingly it doesn't.

## Sambanova

The output content can be found in `Sambanova.out`. I changed the `--ntasks` to 32.