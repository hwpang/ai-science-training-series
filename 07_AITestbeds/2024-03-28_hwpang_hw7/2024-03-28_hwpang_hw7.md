# HW7 hwpang

## Cerebas

The output content can be found in `Cerebas.out`. I used a batch size of 2048. With the larger batch size, the training time becomes shorter compared to the batch size of 1024 during the class. This is because now we need less step to loop through the dataset.

## Graphcore

The output content can be found in `Graphcore.out`. I changed the epochs from 10 to 20 on the mnist example. Compared to the sample output, the test set accuracy increases from 96.85% to 98.16%. This suggests that the model is actually underfitting the dataset, and learning more epochs can help improve the accuracy.

## Groq

The output content can be found in `Groq.out`. I tried customizing in the dummy input as requested by the homework prompt, but I got the following error.

```
Building "bert_tiny"
    ✓ Exporting PyTorch to ONNX   
    ✓ Optimizing ONNX file   
    ✓ Checking for Op support   
    ✓ Converting to FP16   
    ✓ Compiling model   
    ✓ Assembling model   

Woohoo! Saved to ~/.cache/groqflow/bert_tiny
Preprocessing data.
/home/hwpang/miniconda3/envs/groqflow/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for sst contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/sst
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(

Info: No inputs received for benchmark. Using the inputs provided during model compilation.
Running inference on GroqChip.

Error: Model built to always take input of shape
       {'attention_mask': (2, 128), 'input_ids': (2, 128)} but got {'attention_mask': (1, 128), 'input_ids': (1, 128)}
       

▄██████████████▄▐█▄▄▄▄█▌
██████▌▄▌▄▐▐▌███▌▀▀██▀▀
████▄█▌▄▌▄▐▐▌▀███▄▄█▌
▄▄▄▄▄██████████████


Traceback (most recent call last):
  File "/home/hwpang/groqflow/proof_points/natural_language_processing/bert/bert_tiny.py", line 64, in <module>
    evaluate_bert_tiny(**parse_args())
  File "/home/hwpang/groqflow/proof_points/natural_language_processing/bert/bert_tiny.py", line 51, in evaluate_bert_tiny
    compute_performance(
  File "/home/hwpang/groqflow/demo_helpers/demo_helpers/compute_performance.py", line 126, in compute_performance
    groq_performance_result = timed_inference_end_to_end_latency(
  File "/home/hwpang/groqflow/demo_helpers/demo_helpers/compute_performance.py", line 234, in timed_inference_end_to_end_latency
    latency_s = t.timeit(number=1) / len(dataset.x)
  File "/home/hwpang/miniconda3/envs/groqflow/lib/python3.10/timeit.py", line 178, in timeit
    timing = self.inner(it, self.timer)
  File "<timeit-src>", line 6, in inner
  File "/home/hwpang/groqflow/demo_helpers/demo_helpers/compute_performance.py", line 217, in <lambda>
    lambda: result.append(groq_model_inference(dataset, model, task))
  File "/home/hwpang/groqflow/demo_helpers/demo_helpers/compute_performance.py", line 149, in groq_model_inference
    pred = model.run_abunch(dataset.x)
  File "/home/hwpang/groqflow/groqflow/groqmodel/groqmodel.py", line 113, in run_abunch
    self._validate_input_collection(input_collection, "run_abunch")
  File "/home/hwpang/groqflow/groqflow/groqmodel/groqmodel.py", line 228, in _validate_input_collection
    self._validate_inputs(inputs, function_name, True)
  File "/home/hwpang/groqflow/groqflow/groqmodel/groqmodel.py", line 246, in _validate_inputs
    tensor_helpers.check_shapes_and_dtypes(
  File "/home/hwpang/miniconda3/envs/groqflow/lib/python3.10/site-packages/onnxflow/common/tensor_helpers.py", line 36, in check_shapes_and_dtypes
    raise exp.Error(msg)
onnxflow.common.exceptions.Error

```

## Sambanova

The output content can be found in `Sambanova.out`. I changed the `--ntasks` to 32.